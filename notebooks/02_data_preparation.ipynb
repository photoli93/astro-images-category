{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de9608c",
   "metadata": {},
   "source": [
    "# Space Images Classifier - Using Kaggle dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/abhikalpsrivastava15/space-images-category?utm_source=chatgpt.com\n",
    "\n",
    "### This notebook aims for data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62abb3",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f8f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "# Add the root folder to Python's module search path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\"))) \n",
    "# Import the project configuration\n",
    "from config import DEVICE, ORIGINAL_DATA_PATH, OUTPUT_PATH, IMG_SIZE, BATCH_SIZE, NUM_WORKERS, SEED, TRAIN_RATIO, VAL_RATIO, TEST_RATIO\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c74660",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d66286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Val/Test folders (Only executed once)\n",
    "\n",
    "def create_train_val_test_split(original_path, output_path, train_ratio, val_ratio, test_ratio, seed=42):\n",
    "    \"\"\"\n",
    "    Create stratified train/validation/test split\n",
    "    \n",
    "    This function:\n",
    "    1. Preserves class distribution in all splits\n",
    "    2. Ensures no overlapping between train/val/test\n",
    "    3. Creates separate directories for each split\n",
    "    4. Is reproducible (same split every time with same seed)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not original_path.exists():\n",
    "        print(f\"Error : Dataset not found at {original_path}\")\n",
    "        print(\"Please download from: https://www.kaggle.com/datasets/abhikalpsrivastava15/space-images-category\")\n",
    "        return False\n",
    "    \n",
    "    # Check if split already exists\n",
    "    if output_path.exists():\n",
    "        response = input(f\"{output_path} already exists. Recreate split? [YES/NO]: \")\n",
    "        if response.lower() != 'yes':\n",
    "            print(\"Using existing split\")\n",
    "            return True\n",
    "        shutil.rmtree(output_path)\n",
    "    \n",
    "    # Get all classes\n",
    "    classes = sorted([d.name for d in original_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if not classes:\n",
    "        print(\"Error : No class folders found in dataset\")\n",
    "        return False\n",
    "    \n",
    "    print('=' * 80)\n",
    "    print(f\"Found {len(classes)} classes: {classes}\")\n",
    "\n",
    "    # Create output directories\n",
    "    train_path = output_path / \"train\"\n",
    "    val_path = output_path / \"validation\"\n",
    "    test_path = output_path / \"test\"\n",
    "    \n",
    "    for split_path in [train_path, val_path, test_path]:\n",
    "        for cls in classes:\n",
    "            (split_path / cls).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Created directory structure\")\n",
    "    \n",
    "    # Split each class\n",
    "    split_summary = []\n",
    "    \n",
    "    print(f\"Splitting images (seed={seed} for reproducibility)\")\n",
    "\n",
    "    # Loop through classes with progress bar\n",
    "    for cls in tqdm(classes, desc=\"Processing classes\"):\n",
    "        cls_path = original_path / cls\n",
    "        \n",
    "        # Get all images\n",
    "        images = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "            images.extend(list(cls_path.glob(ext)))\n",
    "        \n",
    "        if not images:\n",
    "            print(f\"No images found for class: {cls}\")\n",
    "            continue\n",
    "        \n",
    "        # Shuffle with seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        images = np.array(images)\n",
    "        np.random.shuffle(images)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        n_test = n_total - n_train - n_val\n",
    "        \n",
    "        # Split images\n",
    "        train_images = images[:n_train]\n",
    "        val_images = images[n_train:n_train+n_val]\n",
    "        test_images = images[n_train+n_val:]\n",
    "        \n",
    "        # Copy images to respective folders\n",
    "        for img in train_images:\n",
    "            shutil.copy2(img, train_path / cls / img.name)\n",
    "        \n",
    "        for img in val_images:\n",
    "            shutil.copy2(img, val_path / cls / img.name)\n",
    "        \n",
    "        for img in test_images:\n",
    "            shutil.copy2(img, test_path / cls / img.name)\n",
    "        \n",
    "        split_summary.append({\n",
    "            'class': cls,\n",
    "            'total': n_total,\n",
    "            'train': n_train,\n",
    "            'validation': n_val,\n",
    "            'test': n_test,\n",
    "            'train_pct': n_train/n_total*100,\n",
    "            'val_pct': n_val/n_total*100,\n",
    "            'test_pct': n_test/n_total*100\n",
    "        })\n",
    "    \n",
    "    # Display summary\n",
    "    df_summary = pd.DataFrame(split_summary)\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Overview split summary\")\n",
    "    print('-' * 80)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    print('=' * 80)\n",
    "\n",
    "    # Save summary\n",
    "    df_summary.to_csv(output_path / 'split_summary.csv', index=False)\n",
    "    print(f\"Split summary saved: {output_path / 'split_summary.csv'}\")\n",
    "    \n",
    "    # Verify no overlap\n",
    "    print(\"Verifying that there is no data leaking\")\n",
    "    train_files = set([f.name for f in (train_path / classes[0]).glob(\"*\")])\n",
    "    val_files = set([f.name for f in (val_path / classes[0]).glob(\"*\")])\n",
    "    test_files = set([f.name for f in (test_path / classes[0]).glob(\"*\")])\n",
    "    \n",
    "    overlap_train_val = train_files & val_files\n",
    "    overlap_train_test = train_files & test_files\n",
    "    overlap_val_test = val_files & test_files\n",
    "    \n",
    "    if overlap_train_val or overlap_train_test or overlap_val_test:\n",
    "        print(\"Warning : Data leaking detected\")\n",
    "    else:\n",
    "        print(\"Verified : No overlap between train/val/test sets\")\n",
    "        print('=' * 80)\n",
    "    \n",
    "    # Visualize split distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Absolute counts\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0].bar(x - width, df_summary['train'], width, label='Train')\n",
    "    axes[0].bar(x, df_summary['validation'], width, label='Validation')\n",
    "    axes[0].bar(x + width, df_summary['test'], width, label='Test')\n",
    "    axes[0].set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Number of images', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Image Distribution across splits', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(classes, rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Percentages\n",
    "    axes[1].bar(x - width, df_summary['train_pct'], width, label='Train')\n",
    "    axes[1].bar(x, df_summary['val_pct'], width, label='Validation')\n",
    "    axes[1].bar(x + width, df_summary['test_pct'], width, label='Test')\n",
    "    axes[1].set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Percentage Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(classes, rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].axhline(y=70, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].axhline(y=15, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'split_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('=' * 80)\n",
    "    print(f\"Split visualization saved: {output_path / 'split_visualization.png'}\")\n",
    "    print(\"Data split complete with no data leaking\")\n",
    "    print('=' * 80)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8f7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing split\n"
     ]
    }
   ],
   "source": [
    "# Create the split\n",
    "split_success = create_train_val_test_split(\n",
    "    ORIGINAL_DATA_PATH, \n",
    "    OUTPUT_PATH, \n",
    "    TRAIN_RATIO, \n",
    "    VAL_RATIO, \n",
    "    TEST_RATIO, \n",
    "    SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567a1fa",
   "metadata": {},
   "source": [
    "# Custom PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ff135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceImageDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for space images\"\"\"\n",
    "    \n",
    "    # Call the initializer/constructor to set up everything the object needs\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Load all image paths and labels\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = self.root_dir / cls\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                for img_path in cls_path.glob(ext):\n",
    "                    self.samples.append((img_path, self.class_to_idx[cls]))\n",
    "    \n",
    "    # Number of samples (Needed for batching)\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # Called when PyTorch wants an image\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image and converts BGR to RGB\n",
    "        image = cv2.imread(str(img_path))\n",
    "        \n",
    "        if image is None:\n",
    "            # Skip or return a black image\n",
    "            image = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8a3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    # ToPILImage() because of cv2.imread() defined earlier (returned a NumPy array but need a PIL Image)\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize for faster training, better converging\n",
    "    # Pre-trained models like ResNet, VGG or EfficientNet were trained on ImageNet and these number are\n",
    "    # the channel-wise mean and std of ImageNet images\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b2d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dynamic config json file created with NUM_CLASSES, class_names, split_success variables added\n",
      "Datasets created:\n",
      "  Training:     772 images\n",
      "  Validation:   163 images\n",
      "  Test:         172 images\n",
      "================================================================================\n",
      "Classes (6): ['constellation', 'cosmos space', 'galaxies', 'nebula', 'planets', 'stars']\n",
      "DataLoaders created (batch_size=32)\n",
      "================================================================================\n",
      "Class weights calculated\n",
      "Class weights tensor: tensor([1.0052, 1.1092, 0.7798, 1.0904, 1.0461, 1.0546], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create datasets\n",
    "    train_dataset = SpaceImageDataset(OUTPUT_PATH / \"train\", transform=train_transforms)\n",
    "    val_dataset = SpaceImageDataset(OUTPUT_PATH / \"validation\", transform=val_test_transforms)\n",
    "    test_dataset = SpaceImageDataset(OUTPUT_PATH / \"test\", transform=val_test_transforms)\n",
    "    \n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    ROOT_PATH = Path(\"..\")\n",
    "    # Path to save JSON\n",
    "    CONFIG_JSON_PATH = ROOT_PATH / \"config_dynamic.json\"\n",
    "\n",
    "    # Save to a JSON file for scalability\n",
    "    data = {\n",
    "        \"NUM_CLASSES\": NUM_CLASSES,\n",
    "        \"class_names\": class_names,\n",
    "        \"split_success\": split_success\n",
    "    }\n",
    "    with open(CONFIG_JSON_PATH, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Dynamic config json file created with NUM_CLASSES, class_names, split_success variables added\")\n",
    "    \n",
    "    print(f\"Datasets created:\")\n",
    "    print(f\"  Training:   {len(train_dataset):5d} images\")\n",
    "    print(f\"  Validation: {len(val_dataset):5d} images\")\n",
    "    print(f\"  Test:       {len(test_dataset):5d} images\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Classes ({NUM_CLASSES}): {class_names}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        # Randomized order for training\n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        # Optimization for faster GPU transfer on M1 MacBook Chip\n",
    "        pin_memory=True if DEVICE.type == 'mps' else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if DEVICE.type == 'mps' else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if DEVICE.type == 'mps' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"DataLoaders created (batch_size={BATCH_SIZE})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate class weights to balance imbalanced datasets\n",
    "    labels = [label for _, label in train_dataset.samples]\n",
    "    class_weights_array = class_weight.compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "    )\n",
    "\n",
    "    # Convert weights to a PyTorch tensor on the current device (mps here)\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights_array).to(DEVICE)\n",
    "    torch.save(class_weights_tensor, \"models/class_weights_tensor.pth\")\n",
    "    print(\"Class weights calculated\")\n",
    "    print(\"Class weights tensor:\", class_weights_tensor)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Cannot create datasets or DataLoaders\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c126990",
   "metadata": {},
   "source": [
    "# End of notebook 2 - Data preparation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
