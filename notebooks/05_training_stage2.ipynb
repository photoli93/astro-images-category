{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7036d689",
   "metadata": {},
   "source": [
    "# Space Images Classifier - Using Kaggle dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/abhikalpsrivastava15/space-images-category?utm_source=chatgpt.com\n",
    "\n",
    "### Notebook 5 - Training stage 2 (Unfrozen backbone for fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255efa75",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937ec054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the root folder to Python's module search path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\"))) \n",
    "# Import the project configuration\n",
    "from config import DEVICE, OUTPUT_PATH, BATCH_SIZE, NUM_WORKERS, EPOCHS_STAGE2, LEARNING_RATE_STAGE2\n",
    "from models import SpaceClassifier\n",
    "from train_utils import train_epoch, validate\n",
    "from datasets import SpaceImageDataset, train_transforms, val_test_transforms\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7aebf",
   "metadata": {},
   "source": [
    "# Load callback/config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5766abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path(\"..\")\n",
    "# Path to load JSON\n",
    "CONFIG_JSON_PATH = ROOT_PATH / \"config_dynamic.json\"\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_JSON_PATH) as f:\n",
    "        dynamic_config = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    dynamic_config = {}\n",
    "    \n",
    "NUM_CLASSES = dynamic_config.get(\"NUM_CLASSES\", 0)\n",
    "class_names = dynamic_config.get(\"class_names\", [])\n",
    "split_success = dynamic_config.get(\"split_success\", 5)\n",
    "\n",
    "# Path to the saved tensor\n",
    "WEIGHTS_PATH = Path(\"models\") / \"class_weights_tensor.pth\"\n",
    "\n",
    "# Load the tensor\n",
    "class_weights_tensor = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc9a1b",
   "metadata": {},
   "source": [
    "# Training stage 2 with unfrozen backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb148444",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02640134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Model created: EfficientNet-B0\n",
      "Device: mps\n",
      "Total parameters: 4,796,290\n",
      "Trainable parameters: 788,742\n",
      "--------------------------------------------------------------------------------\n",
      "Best model from stage 1 loaded\n",
      "History of the stage 1 loaded\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SpaceClassifier(NUM_CLASSES, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model created: EfficientNet-B0\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/stage1_best.pth'))\n",
    "print(\"Best model from stage 1 loaded\")\n",
    "# Load history of the stage1\n",
    "history_stage1 = torch.load('models/history_stage1_del_img.pt')\n",
    "print(\"History of the stage 1 loaded\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fb85d",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbc53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created from ../space_images_split/train\n",
      "   -> 765 valid images\n",
      "7 images skipped (see ../space_images_split/train/skipped_images.txt)\n",
      "Dataset created from ../space_images_split/validation\n",
      "   -> 161 valid images\n",
      "2 images skipped (see ../space_images_split/validation/skipped_images.txt)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpaceImageDataset(OUTPUT_PATH / \"train\", transform=train_transforms)\n",
    "val_dataset = SpaceImageDataset(OUTPUT_PATH / \"validation\", transform=val_test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72fcda",
   "metadata": {},
   "source": [
    "## Create Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f157dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE.type == 'mps' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE.type == 'mps' else False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0963a13",
   "metadata": {},
   "source": [
    "## Training the model - Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27350b3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SpaceClassifier.unfreeze_backbone_stage() got multiple values for argument 'stage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights_tensor, label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # Unfreeze backbone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model.unfreeze_backbone()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(f\"Backbone unfrozen\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# === Unfreeze for Stage 2 ===\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfreeze_backbone_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # Add weight_decay in order to add L2 regularization > it penalizes large weights\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_STAGE2, weight_decay=1e-3)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# === Differential Learning Rates ===\u001b[39;00m\n\u001b[1;32m     20\u001b[0m backbone_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n",
      "\u001b[0;31mTypeError\u001b[0m: SpaceClassifier.unfreeze_backbone_stage() got multiple values for argument 'stage'"
     ]
    }
   ],
   "source": [
    "if split_success and history_stage1:\n",
    "    # Loss and optimizer\n",
    "    # Add label_smoothing argument to help generalization and better handle noisy labels\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "    # # Unfreeze backbone\n",
    "    # model.unfreeze_backbone()\n",
    "    # print(f\"Backbone unfrozen\")\n",
    "\n",
    "    # # Unfreeze partial backbone (only the last block)\n",
    "    # model.unfreeze_backbone_partial()\n",
    "\n",
    "    # Unfreeze for stage 2\n",
    "    model.unfreeze_backbone_stage(stage=2)\n",
    "\n",
    "    # # Add weight_decay in order to add L2 regularization > it penalizes large weights\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_STAGE2, weight_decay=1e-3)\n",
    "\n",
    "    # === Differential Learning Rates ===\n",
    "    backbone_params = [p for n, p in model.backbone.named_parameters() if \"features\" in n and p.requires_grad]\n",
    "    classifier_params = [p for n, p in model.backbone.named_parameters() if \"classifier\" in n and p.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': backbone_params, 'lr': 1e-6, 'weight_decay': 1e-3},\n",
    "        {'params': classifier_params, 'lr': 1e-4, 'weight_decay': 1e-3}\n",
    "    ])\n",
    "\n",
    "    # Automatically reduces the learning rate by 50% if validation loss stops improving for 5 epochs\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Starting Stage 2 Training\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Estimated time on M1: 20-40 minutes\\n\")\n",
    "    print(\"=\" * 80)   \n",
    "\n",
    "    history_stage2 = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    PATIENCE = 6\n",
    "    \n",
    "    for epoch in range(EPOCHS_STAGE2):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS_STAGE2}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history_stage2['train_loss'].append(train_loss)\n",
    "        history_stage2['train_acc'].append(train_acc)\n",
    "        history_stage2['val_loss'].append(val_loss)\n",
    "        history_stage2['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'models/final_best.pth')\n",
    "            print(f\"Best model saved! (Val Acc: {val_acc*100:.2f}%)\")\n",
    "            # Save stage 2 logs to the disk\n",
    "            torch.save(history_stage2, 'models/history_stage2.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered (patience={PATIENCE})\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\nStage 2 fine-tuning complete!\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('models/final_best.pth'))\n",
    "\n",
    "else:\n",
    "    history_stage2 = None\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d57d7",
   "metadata": {},
   "source": [
    "# End of notebook 5 - Training stage 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
